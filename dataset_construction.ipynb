{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bf9280-0675-4823-be84-26bd578f5651",
   "metadata": {},
   "source": [
    "---\n",
    "title: Constructing an AR Dataset\n",
    "\n",
    "author: Jimmy Butler\n",
    "subtitle: A notebook to demonstrate workflows to ascribe characteristics and impacts to landfalling ARs in a catalog of AR storm events.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58784e3c-b659-4933-9890-1ec79d2386a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de41843-3815-4963-a674-7ad4d8bf0f12",
   "metadata": {},
   "source": [
    "First, we'll load up any packages and modules we might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d1f096-d0e0-417e-96e4-716f72b2f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import earthaccess\n",
    "import ray\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2328f37d-f018-4491-a051-9f18f86e6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load internal helper modules\n",
    "\n",
    "from utils.loading_utils import load_ais, load_cell_areas, EarthdataGatekeeper\n",
    "from utils.attribute_utils import *\n",
    "from utils.compute_attributes_streaming import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4397e4a-a806-4631-ab15-b67a2afca1a2",
   "metadata": {},
   "source": [
    "Next, let's load up the catalog. We'll be using a subset of the full version for now (the first 250 landfalling storms, out of ~3000 total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d523d4-7a80-416e-9261-2b1ff6c07b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pytables'.  Use pip or conda to install pytables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/compat/_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tables'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m total_storms \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcatalog/subset_storms.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#landfalling_storms = total_storms[total_storms.is_landfalling]\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/pytables.py:439\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_buf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 439\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# so delegate to the iterator\u001b[39;00m\n\u001b[1;32m    442\u001b[0m auto_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/pytables.py:579\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat is not a defined argument for HDFStore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 579\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m complib \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tables\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mall_complibs:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplib only supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtables\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mall_complibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m compression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pytables'.  Use pip or conda to install pytables."
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "total_storms = pd.read_hdf('catalog/subset_storms.h5')\n",
    "#landfalling_storms = total_storms[total_storms.is_landfalling]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad710537-b824-407f-a69a-264de1ea610c",
   "metadata": {},
   "source": [
    "## Catalog Overview\n",
    "\n",
    "Before we get started with the masking workflow, we can first take a quick look at the catalog. The catalog is a clusterig of AR pixels identified in the [Wille (2021) catalog](https://doi.org/10.5281/zenodo.4009663), which takes an Eulerian approach to identifying atmoshperic rivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e246e7-d079-4a49-b8bd-b56df7fb1901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f00a31d-74f4-41e2-a3de-4650ff30029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 8\n",
    "miniters = num_cpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91965865-fee9-42f7-a174-5d00ef52325b",
   "metadata": {},
   "source": [
    "We're going to be processing the data sequentially in chunks, with each chunk being processed in parallel. This is to reduce any memory pressure that we might incur by processing the whole list of storms in the same parallel instance/ray cluster. When I attempted to loop through the whole list of storms in parallel, I noticed that some iterations of the loop were taking extremely long to process (much longer than if I had executed the loop on a smaller chunk of storms), and then the server outright crashed. I suspect this is due to memory pressure, and so we start up a ray instance to process each chunk of the dataset individually, clearing the ray instance for the next chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa74c5c-683e-444d-aade-86a9b70f5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_list = np.arange(landfalling_storms.shape[0])\n",
    "chunk_size = 500\n",
    "chunk_inds_lst = [indices_list[i:i + chunk_size] for i in range(0, len(indices_list), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b41e04d-1292-48f6-903d-efc4f0467683",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dois = {'climatology': '10.5067/5ESKGQTZG7FO',\n",
    "             'T2M_TQV_SLP': '10.5067/3Z173KIE2TPD',\n",
    "             'VFLXQV_PRECIP': '10.5067/Q5GVUVUIVGO7',\n",
    "             'V850': '10.5067/VJAFPLI1CSIV',\n",
    "             'OMEGA': '10.5067/QBZ6MG944HW0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bcefe-94da-4669-ab0a-fde7d81bb05b",
   "metadata": {},
   "source": [
    "Now, let's load up the auxiliary datasets that will help us compute our relevant AR characteristics and impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d156fe-350f-428b-af47-e622e9b45ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_mask = loading_utils.load_ais()\n",
    "ais_mask = ais_mask.assign_coords(lat=ais_mask.lat.round(5), \n",
    "                                  lon=ais_mask.lon.round(5))\n",
    "\n",
    "cell_areas = loading_utils.load_cell_areas()\n",
    "cell_areas = cell_areas.assign_coords(lat=cell_areas.lat.round(5), \n",
    "                                      lon=cell_areas.lon.round(5)) # this is to avoid -0 not matching 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d1742-10f5-4e22-bc23-81ce4da61097",
   "metadata": {},
   "source": [
    "Now, let's compute our climatologies, which we will later use to compute temperature and IWV anomalies. We'll just compute the climatology dataset and store it memory, partially because it's not that huge, and also because if we leave it to be executed until it's time to compute anomalies for the individual ARs, we may be repeating our anomaly computations multiple times. This will help us save on time.\n",
    "\n",
    ":::{attention}\n",
    "Either we can run the next cell to create the climatology dataset, which should take about 15 minutes to do. Or, you could run this just once, save it, and just load it up from disk for future runs.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5e6723-d8e7-43e8-8712-23847473ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatology_ds = xr.open_dataset('climatology.nc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b442fbe-008e-42e0-9271-f3ddd9f437ae",
   "metadata": {},
   "source": [
    "Now, we write a function to setup our Ray instance to do our parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfd81ed-bdc2-4273-b025-743e697f9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ray(func_vars_dict):\n",
    "    '''\n",
    "    Function that serves to stop the previous chunk's ray cluster, clear its memory,\n",
    "        and start a new one for a new chunk. Also initializes the gatekeeper for\n",
    "        making open requests to Earthdata, and stores bigger objects into Ray's\n",
    "        object store for more efficient sharing of these objects across clusters.\n",
    "\n",
    "    Inputs:\n",
    "        func_vars_dict (dict of dict): outer layer of dictionary keys are names of variables in\n",
    "            MERRA-2 datasets, inner layer are names of desired aggregates of those variables as keys,\n",
    "            along with their aggregation functions as values.\n",
    "    '''\n",
    "    \n",
    "    # shutdown and restart the ray cluster, to clear memory\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    \n",
    "    ray.init(num_cpus=num_cpus, logging_level='ERROR', \n",
    "         _metrics_export_port=None, include_dashboard=False, \n",
    "         log_to_driver=False, runtime_env={'py_modules': [attribute_utils, loading_utils, st_dbscan]})\n",
    "\n",
    "    climatology_ref = ray.put(climatology_ds)\n",
    "    cell_areas_ref = ray.put(cell_areas)\n",
    "    func_vars_ref = ray.put(func_vars_dict)\n",
    "\n",
    "    return climatology_ref, cell_areas_ref, func_vars_ref\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd222b-7ed0-442a-ba9e-dbe4ff8222ee",
   "metadata": {},
   "source": [
    "## Masking Quantities from `10.5067/3Z173KIE2TPD`\n",
    "\n",
    "This includes IWV, IWV anomaly, 2m-temperature, 2m-temperature anomaly, and sea level pressure quantities.\n",
    "\n",
    "We arrange our `func_vars_dict` variable in the following way so that, if there are multiple quantities and aggregations we wish to compute on the same variable DataArray, we compute all of them on the same streamed object instead of throwing it away and restreaming for each additional quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a07fddf-874f-4427-a077-b032a31a6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/antarctic_ars/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "[2025-12-05 05:11:27,397 E 97 441] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-05 05:20:09,315 E 97 5430] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-05 05:28:46,962 E 97 10900] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-05 05:37:17,762 E 97 15830] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-05 05:46:32,686 E 97 21319] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-05 05:55:33,155 E 97 26437] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-05 06:04:16,786 E 97 31252] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "func_vars_dict1 = {'T2M': {'max_T2m_ais': lambda storm_da, var_da, area_da: compute_max_intensity(storm_da, var_da, area_da, ais_mask),\n",
    "                          'max_T2M_anomaly_ais': lambda storm_da, var_da, area_da: compute_max_intensity(storm_da, var_da, area_da, ais_mask)},\n",
    "                  'TQV': {'avg_IWV_ais': lambda storm_da, var_da, area_da: compute_average(storm_da, var_da, area_da, ais_mask),\n",
    "                          'max_IWV_ais': lambda storm_da, var_da, area_da: compute_max_intensity(storm_da, var_da, area_da, ais_mask),\n",
    "                          'max_IWV_anomaly_ais': lambda storm_da, var_da, area_da: compute_max_intensity(storm_da, var_da, area_da, ais_mask)},\n",
    "                  'SLP': {'max_ocean_SLP_gradient': lambda storm_da, var_da, area_da: compute_max_SLPgrad(storm_da, var_da, area_da, ais_mask)}\n",
    "                 }\n",
    "\n",
    "data_doi = data_dois['T2M_TQV_SLP']\n",
    "progress_log_path = 'T2M_TQV_SLP_logs'\n",
    "\n",
    "missing_days1 = []\n",
    "parallel_results1 = []\n",
    "\n",
    "for i, chunk_inds in enumerate(chunk_inds_lst):\n",
    "\n",
    "    storms_chunk = landfalling_storms.iloc[chunk_inds]\n",
    "    climatology_ref, cell_areas_ref, func_vars_ref = setup_ray(func_vars_dict1)\n",
    "    gatekeeper = EarthdataGatekeeper.remote()\n",
    "\n",
    "    result_refs = []\n",
    "    for index, row in storms_chunk.iterrows():\n",
    "        result_refs.append(compute_summaries.remote(\n",
    "            row.data_array,\n",
    "            func_vars_ref,\n",
    "            cell_areas_ref,\n",
    "            data_doi,\n",
    "            gatekeeper=gatekeeper,\n",
    "            climatology_ds=climatology_ref))\n",
    "\n",
    "\n",
    "    with open(f'{progress_log_path}/streaming_progress{i+1}.log', \"w\") as log_file:\n",
    "        parallel_results = []\n",
    "        missing_days = []\n",
    "        for ref in tqdm(result_refs, total=len(result_refs), file=log_file, miniters=miniters):\n",
    "            results = ray.get(ref)\n",
    "            parallel_results.append(results[0])\n",
    "            missing_days.append(results[1])\n",
    "\n",
    "    parallel_results1 = parallel_results1 + parallel_results\n",
    "    missing_days1 = missing_days1 + missing_days\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed5d08d-9eae-4a13-8125-7e7767e86364",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = [key for quantity_dict in func_vars_dict1.values() for key in quantity_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66548295-51c3-450c-bed5-6fbd50b207d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(parallel_results1, columns=labels1)\n",
    "df1['T2M_TQV_SLP_missing'] = missing_days1\n",
    "df1.to_csv('streamed_datasets/streamed_T2M_TQV_SLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deced9d6-0614-4a2a-b7be-8ffb2d45b7fa",
   "metadata": {},
   "source": [
    "## Masking Quantities from `10.5067/Q5GVUVUIVGO7`\n",
    "\n",
    "This includes IVT and precipitation quantities (rainfall, snowfall). Starting with IVT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0729100-789d-492b-9068-c1f3b7a0027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_vars_dict2 = {'VFLXQV': {'avg_vIVT_ais': lambda storm_da, var_da, area_da: compute_average(storm_da, -var_da, area_da, ais_mask),\n",
    "                             'max_vIVT_ais': lambda storm_da, var_da, area_da: compute_max_intensity(storm_da, -var_da, area_da, ais_mask),\n",
    "                             'avg_vIVT': lambda storm_da, var_da, area_da: compute_average(storm_da, -var_da, area_da),\n",
    "                             'max_vIVT': lambda storm_da, var_da, area_da: compute_max_intensity(storm_da, -var_da, area_da)}}\n",
    "data_doi = data_dois['VFLXQV_PRECIP']\n",
    "progress_log_path = 'VFLXQV_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6266c6-4351-4919-a2d8-d9a05be9d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 06:11:06,023 E 97 36130] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m missing_days \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m tqdm(result_refs, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(result_refs), file\u001b[38;5;241m=\u001b[39mlog_file, miniters\u001b[38;5;241m=\u001b[39mminiters):\n\u001b[0;32m---> 25\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     parallel_results\u001b[38;5;241m.\u001b[39mappend(results[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m     missing_days\u001b[38;5;241m.\u001b[39mappend(results[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/envs/antarctic_ars/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/antarctic_ars/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/antarctic_ars/lib/python3.12/site-packages/ray/_private/worker.py:2961\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout, _tensor_transport)\u001b[0m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(object_refs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2957\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2959\u001b[0m     )\n\u001b[0;32m-> 2961\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tensor_transport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_tensor_transport\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/envs/antarctic_ars/lib/python3.12/site-packages/ray/_private/worker.py:998\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout, return_exceptions, skip_deserialization, _tensor_transport)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tensor_transport \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    989\u001b[0m     TensorTransportEnum\u001b[38;5;241m.\u001b[39mOBJECT_STORE,\n\u001b[1;32m    990\u001b[0m     TensorTransportEnum\u001b[38;5;241m.\u001b[39mNIXL,\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    992\u001b[0m ], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, RDT only supports \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_store\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnixl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for tensor transport in ray.get().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    995\u001b[0m )\n\u001b[1;32m    996\u001b[0m serialized_objects: List[\n\u001b[1;32m    997\u001b[0m     serialization\u001b[38;5;241m.\u001b[39mSerializedRayObject\n\u001b[0;32m--> 998\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata, _ \u001b[38;5;129;01min\u001b[39;00m serialized_objects:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3141\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/includes/common.pxi:97\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "missing_days2 = []\n",
    "parallel_results2 = []\n",
    "\n",
    "for i, chunk_inds in enumerate(chunk_inds_lst):\n",
    "\n",
    "    storms_chunk = landfalling_storms.iloc[chunk_inds]\n",
    "    climatology_ref, cell_areas_ref, func_vars_ref = setup_ray(func_vars_dict2)\n",
    "    gatekeeper = EarthdataGatekeeper.remote()\n",
    "\n",
    "    result_refs = []\n",
    "    for index, row in storms_chunk.iterrows():\n",
    "        result_refs.append(compute_summaries.remote(\n",
    "            row.data_array,\n",
    "            func_vars_ref,\n",
    "            cell_areas_ref,\n",
    "            data_doi,\n",
    "            gatekeeper=gatekeeper,\n",
    "            half_hour=True))\n",
    "\n",
    "\n",
    "    with open(f'{progress_log_path}/streaming_progress{i+1}.log', \"w\") as log_file:\n",
    "        parallel_results = []\n",
    "        missing_days = []\n",
    "        for ref in tqdm(result_refs, total=len(result_refs), file=log_file, miniters=miniters):\n",
    "            results = ray.get(ref)\n",
    "            parallel_results.append(results[0])\n",
    "            missing_days.append(results[1])\n",
    "\n",
    "    parallel_results2 = parallel_results2 + parallel_results\n",
    "    missing_days2 = missing_days2 + missing_days\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "%%capture output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11b329-0cf5-4d46-a8c8-e75ee0eb6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = [key for quantity_dict in func_vars_dict2.values() for key in quantity_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289ea33-d11a-410c-bc4e-a040fc12a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(parallel_results2, columns=labels2)\n",
    "df2['VFLXQV_missing'] = missing_days2\n",
    "df2.to_csv('streamed_datasets/streamed_VFLXQV.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc74515-777c-4f32-af98-e72ab5061751",
   "metadata": {},
   "source": [
    "Now moving on to precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeba08d-4603-4876-9e37-eba857c482c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_doi = data_dois['VFLXQV_PRECIP']\n",
    "progress_log_path = 'PRECIP_logs'\n",
    "\n",
    "def precip_func(storm_da, var_da, area_da):\n",
    "    return compute_cumulative(storm_da, var_da, area_da, ais_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf3164-e97e-423b-86e0-c6e8acbe1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days3 = []\n",
    "parallel_results3 = []\n",
    "\n",
    "for i, chunk_inds in enumerate(chunk_inds_lst):\n",
    "\n",
    "    storms_chunk = landfalling_storms.iloc[chunk_inds]\n",
    "    climatology_ref, cell_areas_ref, precip_func_ref = setup_ray(precip_func)\n",
    "    gatekeeper = EarthdataGatekeeper.remote()\n",
    "\n",
    "    result_refs = []\n",
    "    for index, row in storms_chunk.iterrows():\n",
    "        result_refs.append(compute_precip_summaries.remote(\n",
    "            row.data_array, \n",
    "            cell_areas_ref, \n",
    "            precip_func_ref,\n",
    "            data_doi,\n",
    "            gatekeeper=gatekeeper))\n",
    "\n",
    "\n",
    "    with open(f'{progress_log_path}/streaming_progress{i+1}.log', \"w\") as log_file:\n",
    "        parallel_results = []\n",
    "        missing_days = []\n",
    "        for ref in tqdm(result_refs, total=len(result_refs), file=log_file, miniters=miniters):\n",
    "            results = ray.get(ref)\n",
    "            parallel_results.append(results[0])\n",
    "            missing_days.append(results[1])\n",
    "\n",
    "    parallel_results3 = parallel_results3 + parallel_results\n",
    "    missing_days3 = missing_days3 + missing_days\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "%%capture output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e8d47-be4b-4b9a-b911-e2a17edc1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels3 = ['cumulative_rainfall_ais', 'cumulative_snowfall_ais']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d3cd1-69c5-475d-a116-753cba51c708",
   "metadata": {},
   "source": [
    "## Masking Quantities from `10.5067/VJAFPLI1CSIV`\n",
    "\n",
    "This is for poleward 850 hPa wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066933d-35e5-46a7-972f-5b92c35d2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_vars_dict4 = {'V850': {'max_landfalling_v850hPa': lambda storm_da, var_da, area_da: compute_max_landfalling_wind(storm_da, -var_da, area_da, ais_mask),\n",
    "                           'avg_landfalling_v850hPa': lambda storm_da, var_da, area_da: compute_avg_landfalling_wind(storm_da, -var_da, area_da, ais_mask)}}\n",
    "data_doi = data_dois['V850']\n",
    "progress_log_path = 'V850_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd065c-09f0-4381-ac96-4e4ee6248eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days4 = []\n",
    "parallel_results4 = []\n",
    "\n",
    "for i, chunk_inds in enumerate(chunk_inds_lst):\n",
    "\n",
    "    storms_chunk = landfalling_storms.iloc[chunk_inds]\n",
    "    climatology_ref, cell_areas_ref, func_vars_ref = setup_ray(func_vars_dict4)\n",
    "    gatekeeper = EarthdataGatekeeper.remote()\n",
    "\n",
    "    result_refs = []\n",
    "    for index, row in storms_chunk.iterrows():\n",
    "        result_refs.append(compute_summaries.remote(\n",
    "            row.data_array,\n",
    "            func_vars_ref,\n",
    "            cell_areas_ref,\n",
    "            data_doi,\n",
    "            gatekeeper=gatekeeper,\n",
    "            half_hour=True))\n",
    "\n",
    "\n",
    "    with open(f'{progress_log_path}/streaming_progress{i+1}.log', \"w\") as log_file:\n",
    "        parallel_results = []\n",
    "        missing_days = []\n",
    "        for ref in tqdm(result_refs, total=len(result_refs), file=log_file, miniters=miniters):\n",
    "            results = ray.get(ref)\n",
    "            parallel_results.append(results[0])\n",
    "            missing_days.append(results[1])\n",
    "\n",
    "    parallel_results4 = parallel_results4 + parallel_results\n",
    "    missing_days4 = missing_days4 + missing_days\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "%%capture output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe2a3a-aea3-479c-af89-42ba79449a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels4 = [key for quantity_dict in func_vars_dict4.values() for key in quantity_dict.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12901528-c471-4bab-9429-ca44530c2637",
   "metadata": {},
   "source": [
    "## Masking Quantities from `10.5067/QBZ6MG944HW0`\n",
    "\n",
    "This is for omega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcf4d9-817f-4a18-8a74-a95bd41e00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_vars_dict5 = {'OMEGA': {'avg_landfalling_minomega': lambda storm_da, var_da, area_da: compute_avg_landfalling_minomega(storm_da, var_da, area_da, ais_mask)}}\n",
    "data_doi = data_dois['OMEGA']\n",
    "progress_log_path = 'OMEGA_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0d8e4-afe5-42f4-bff5-24672ec67b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days5 = []\n",
    "parallel_results5 = []\n",
    "\n",
    "for i, chunk_inds in enumerate(chunk_inds_lst):\n",
    "\n",
    "    storms_chunk = landfalling_storms.iloc[chunk_inds]\n",
    "    climatology_ref, cell_areas_ref, func_vars_ref = setup_ray(func_vars_dict5)\n",
    "    gatekeeper = EarthdataGatekeeper.remote()\n",
    "\n",
    "    result_refs = []\n",
    "    for index, row in storms_chunk.iterrows():\n",
    "        result_refs.append(compute_summaries.remote(\n",
    "            row.data_array,\n",
    "            func_vars_ref,\n",
    "            cell_areas_ref,\n",
    "            data_doi,\n",
    "            gatekeeper=gatekeeper))\n",
    "\n",
    "\n",
    "    with open(f'{progress_log_path}/streaming_progress{i+1}.log', \"w\") as log_file:\n",
    "        parallel_results = []\n",
    "        missing_days = []\n",
    "        for ref in tqdm(result_refs, total=len(result_refs), file=log_file, miniters=miniters):\n",
    "            results = ray.get(ref)\n",
    "            parallel_results.append(results[0])\n",
    "            missing_days.append(results[1])\n",
    "\n",
    "    parallel_results5 = parallel_results5 + parallel_results\n",
    "    missing_days5 = missing_days5 + missing_days\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "%%capture output5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef97f25-2ae4-421b-a748-34bf866f1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels5 = [key for quantity_dict in func_vars_dict5.values() for key in quantity_dict.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3b8d1-255f-403e-8a4b-7dc25f25b8da",
   "metadata": {},
   "source": [
    "## Remaining Quantities\n",
    "\n",
    "Computing remaining quantities that don't involve streaming any MERRA-2 data, just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f529a-d3ac-4533-a192-73879bd54c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfalling_storms['max_area'] = landfalling_storms['data_array'].apply(lambda x: compute_max_area(x, cell_areas))\n",
    "landfalling_storms['mean_area'] = landfalling_storms['data_array'].apply(lambda x: compute_mean_area(x, cell_areas))\n",
    "landfalling_storms['mean_landfalling_area'] = landfalling_storms['data_array'].apply(lambda x: compute_mean_area(x, cell_areas, ais_mask))\n",
    "landfalling_storms['cumulative_landfalling_area'] = landfalling_storms['data_array'].apply(lambda x: compute_cumulative_spacetime(x, cell_areas, ais_mask))\n",
    "landfalling_storms['duration'] = landfalling_storms['data_array'].apply(compute_duration)\n",
    "landfalling_storms['start_date'] = landfalling_storms['data_array'].apply(add_start_date)\n",
    "landfalling_storms['end_date'] = landfalling_storms['data_array'].apply(add_end_date)\n",
    "landfalling_storms['max_south_extent'] = landfalling_storms['data_array'].apply(compute_max_southward_extent)\n",
    "landfalling_storms['max_elevation_grad'] = landfalling_storms['data_array'].apply(lambda x: compute_max_elevation_grad(x, elevation))\n",
    "\n",
    "region_defs_coarser = {'West': [-150, -30], \n",
    "               'East 1': [-30, 75],\n",
    "               'East 2': [75, -150]}\n",
    "\n",
    "region_masks_coarser = find_region_masks(region_defs_coarser, ais_mask)\n",
    "\n",
    "landfalling_storms['coarser_region'] = landfalling_storms['data_array'].apply(lambda x: find_landfalling_region(x, cell_areas, region_masks_coarser))\n",
    "\n",
    "landfalling_storms['trajectory'] = landfalling_storms['data_array'].apply(extract_trajectory)\n",
    "\n",
    "# add in the merra2 aggregates to the landfalling dataframe\n",
    "landfalling_storms[labels1] = parallel_results1\n",
    "landfalling_storms[labels2] = parallel_results2\n",
    "landfalling_storms[labels3] = parallel_results3\n",
    "landfalling_storms[labels4] = parallel_results4\n",
    "landfalling_storms[labels5] = parallel_results5\n",
    "\n",
    "# add in the missing days\n",
    "landfalling_storms['T2M_TQV_SLP_missing'] = missing_days1\n",
    "landfalling_storms['VFLXQV_missing'] = missing_days2\n",
    "landfalling_storms['PRECIP_missing'] = missing_days3\n",
    "landfalling_storms['V850_missing'] = missing_days4\n",
    "landfalling_storms['OMEGA_missing'] = missing_days5\n",
    "\n",
    "# save the dataframe\n",
    "landfalling_storms.to_hdf(home_dir/'project/dataset/datasets/streamed_landfalling_storm_quantities_df.h5', key='df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
